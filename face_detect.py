"""
Gender and Emotion Detection System

Este sistema realiza detec√ß√£o facial em tempo real com classifica√ß√£o de g√™nero e emo√ß√£o.
Utiliza tr√™s modelos YOLO:
- YOLOv11n-face: Para detec√ß√£o de faces
- Gender Model: Para classifica√ß√£o de g√™nero (female/male)
- Emotion Model: Para classifica√ß√£o de emo√ß√µes (5 categorias)

Autor: Eduardo
Data: 2025
"""

import cv2
import json
import os
import sys
import numpy as np
from ultralytics import YOLO
import supervision as sv

# Defini√ß√£o das classes para classifica√ß√£o
CLASSES_GENDER = ['female', 'male']  # Classes de g√™nero suportadas
CLASSES_EMOTION = ['disgust', 'happy', 'neutral', 'surprise', 'unknown']  # Classes de emo√ß√£o suportadas


def load_config(config_path="face_config.json"):
    """
    Carrega as configura√ß√µes do sistema a partir de um arquivo JSON.
    
    O sistema suporta execu√ß√£o tanto como script Python quanto como execut√°vel compilado.
    Se o arquivo de configura√ß√£o n√£o for encontrado, usa valores padr√£o seguros.
    
    Args:
        config_path (str): Caminho relativo para o arquivo de configura√ß√£o
        
    Returns:
        dict: Dicion√°rio com as configura√ß√µes do sistema contendo:
            - confidence_face: Threshold de confian√ßa para detec√ß√£o de faces
            - confidence_gender: Threshold de confian√ßa para classifica√ß√£o de g√™nero
            - confidence_emotion: Threshold de confian√ßa para classifica√ß√£o de emo√ß√£o
            - resize_resolution: Resolu√ß√£o da janela de exibi√ß√£o
            - source_video: Fonte de v√≠deo (0=webcam, caminho=arquivo)
    """
    # Determinar o caminho base da aplica√ß√£o (compat√≠vel com execut√°veis)
    if getattr(sys, 'frozen', False):
        # Executando como execut√°vel compilado (ex: PyInstaller)
        application_path = os.path.dirname(sys.executable)
    else:
        # Executando como script Python normal
        application_path = os.path.dirname(os.path.abspath(__file__))
    
    config_file_path = os.path.join(application_path, config_path)
    
    try:
        # Tentar carregar configura√ß√µes do arquivo JSON
        with open(config_file_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"‚úÖ Configura√ß√µes carregadas de: {config_file_path}")
        return config
    except FileNotFoundError:
        # Arquivo n√£o encontrado - usar configura√ß√µes padr√£o
        print(f"‚ö†Ô∏è  Arquivo de configura√ß√£o {config_file_path} n√£o encontrado. Usando configura√ß√µes padr√£o.")
        return {
            "confidence_face": 0.3,
            "confidence_gender": 0.3,
            "confidence_emotion": 0.3,
            "resize_resolution": "1280x720",
            "source_video": 0
        }
    except json.JSONDecodeError as e:
        # Arquivo JSON malformado - usar configura√ß√µes padr√£o
        print(f"‚ùå Erro ao decodificar o arquivo {config_file_path}: {e}. Usando configura√ß√µes padr√£o.")
        return {
            "confidence_face": 0.3,
            "confidence_gender": 0.3,
            "confidence_emotion": 0.3,
            "resize_resolution": "1280x720",
            "source_video": 0
        }

def corner_rect(img, bbox, l=30, t=5, rt=1,
                color_rect=(255, 0, 255), color_corners=(0, 255, 0)):
    """
    Draw a rectangle with highlighted corners.
    
    Args:
        img: Image to draw on
        bbox: Bounding box coordinates [x, y, w, h]
        l: Length of corner lines
        t: Thickness of corner lines
        rt: Thickness of main rectangle
        color_rect: Color of the main rectangle (BGR)
        color_corners: Color of the corner lines (BGR)
    
    Returns:
        Image with drawn rectangle and corners
    """
    x, y, w, h = bbox
    x1, y1 = x + w, y + h
    
    # Draw main rectangle
    if rt != 0:
        cv2.rectangle(img, bbox, color_rect, rt)
    
    # Draw corners
    # Top Left
    cv2.line(img, (x, y), (x + l, y), color_corners, t)
    cv2.line(img, (x, y), (x, y + l), color_corners, t)
    # Top Right
    cv2.line(img, (x1, y), (x1 - l, y), color_corners, t)
    cv2.line(img, (x1, y), (x1, y + l), color_corners, t)
    # Bottom Left
    cv2.line(img, (x, y1), (x + l, y1), color_corners, t)
    cv2.line(img, (x, y1), (x, y1 - l), color_corners, t)
    # Bottom Right
    cv2.line(img, (x1, y1), (x1 - l, y1), color_corners, t)
    cv2.line(img, (x1, y1), (x1, y1 - l), color_corners, t)
    
    return img

def put_text_rect(img, text, pos, scale=3, thickness=3,
                  color_text=(255, 255, 255), color_rect=(255, 0, 255),
                  font=cv2.FONT_HERSHEY_PLAIN, offset=10,
                  border=None, color_border=(0, 255, 0)):
    """
    Put text on image with rectangle background.
    
    Args:
        img: Image to draw on
        text: Text to display
        pos: Starting position (x, y)
        scale: Text scale
        thickness: Text thickness
        color_text: Text color (BGR)
        color_rect: Rectangle background color (BGR)
        font: OpenCV font type
        offset: Padding around text
        border: Border thickness (None for no border)
        color_border: Border color (BGR)
    
    Returns:
        tuple: (Modified image, Rectangle coordinates [x1, y2, x2, y1])
    """
    ox, oy = pos
    (w, h), _ = cv2.getTextSize(text, font, scale, thickness)
    
    # Calculate rectangle coordinates
    x1, y1 = ox - offset, oy + offset
    x2, y2 = ox + w + offset, oy - h - offset
    
    # Draw background rectangle
    cv2.rectangle(img, (x1, y1), (x2, y2), color_rect, cv2.FILLED)
    
    # Draw border if specified
    if border is not None:
        cv2.rectangle(img, (x1, y1), (x2, y2), color_border, border)
    
    # Draw text
    cv2.putText(img, text, (ox, oy), font, scale, color_text, thickness)
    
    return img, [x1, y2, x2, y1]

def load_models():
    """
    Carrega os tr√™s modelos YOLO necess√°rios para o sistema:
    1. Modelo de detec√ß√£o facial (YOLOv11n-face)
    2. Modelo de classifica√ß√£o de g√™nero
    3. Modelo de classifica√ß√£o de emo√ß√£o
    
    Returns:
        tuple: (model_face, model_gender, model_emotion)
            - model_face: Modelo YOLO para detec√ß√£o de faces
            - model_gender: Modelo YOLO para classifica√ß√£o de g√™nero
            - model_emotion: Modelo YOLO para classifica√ß√£o de emo√ß√£o
            
    Raises:
        FileNotFoundError: Se algum dos arquivos de modelo n√£o for encontrado
        Exception: Se houver erro ao carregar os modelos
    """
    # Determinar o caminho base para localizar os modelos
    if getattr(sys, 'frozen', False):
        # Executando como execut√°vel compilado
        application_path = os.path.dirname(sys.executable)
    else:
        # Executando como script Python
        application_path = os.path.dirname(os.path.abspath(__file__))
    
    models_dir = os.path.join(application_path, "models")
    
    try:
        print("üîÑ Carregando modelos YOLO...")
        
        # Carregar modelo de detec√ß√£o facial
        face_model_path = os.path.join(models_dir, "yolov11n-face.pt")
        model_face = YOLO(face_model_path)
        print(f"‚úÖ Modelo de detec√ß√£o facial carregado: {face_model_path}")
        
        # Carregar modelo de classifica√ß√£o de g√™nero
        gender_model_path = os.path.join(models_dir, "gender.pt")
        model_gender = YOLO(gender_model_path)
        print(f"‚úÖ Modelo de g√™nero carregado: {gender_model_path}")
        
        # Carregar modelo de classifica√ß√£o de emo√ß√£o
        emotion_model_path = os.path.join(models_dir, "emotion.pt")
        model_emotion = YOLO(emotion_model_path)
        print(f"‚úÖ Modelo de emo√ß√£o carregado: {emotion_model_path}")
        
        return model_face, model_gender, model_emotion
        
    except FileNotFoundError as e:
        print(f"‚ùå Erro: Arquivo de modelo n√£o encontrado - {e}")
        print("üí° Certifique-se de que todos os modelos est√£o na pasta 'models/':")
        print("   - yolov11n-face.pt")
        print("   - gender.pt")
        print("   - emotion.pt")
        raise
    except Exception as e:
        print(f"‚ùå Erro ao carregar modelos: {e}")
        raise

def extract_face_region(frame, bbox):
    """
    Extrai a regi√£o da face detectada do frame original.
    
    Esta fun√ß√£o √© crucial para o pipeline de classifica√ß√£o, pois os modelos
    de g√™nero e emo√ß√£o precisam da regi√£o da face isolada para fazer
    predi√ß√µes precisas.
    
    Args:
        frame (numpy.ndarray): Frame original da c√¢mera/v√≠deo
        bbox (list/tuple): Coordenadas da bounding box [x1, y1, x2, y2]
        
    Returns:
        numpy.ndarray or None: Regi√£o da face extra√≠da ou None se inv√°lida
    """
    # Converter coordenadas para inteiros
    x1, y1, x2, y2 = map(int, bbox)
    
    # Obter dimens√µes do frame para valida√ß√£o
    h, w = frame.shape[:2]
    
    # Garantir que as coordenadas est√£o dentro dos limites v√°lidos do frame
    # Isso previne erros de √≠ndice e garante extra√ß√£o segura
    x1, y1 = max(0, x1), max(0, y1)
    x2, y2 = min(w, x2), min(h, y2)
    
    # Verificar se a bounding box √© v√°lida (tem √°rea positiva)
    if x2 > x1 and y2 > y1:
        # Extrair e retornar a regi√£o da face
        face_region = frame[y1:y2, x1:x2]
        return face_region
    
    # Retornar None se a bounding box for inv√°lida
    return None

def classify_face(model_gender, model_emotion, face_region, confidence_gender, confidence_emotion):
    """
    Realiza a classifica√ß√£o de g√™nero e emo√ß√£o em uma regi√£o facial extra√≠da.
    
    Esta fun√ß√£o aplica os modelos de classifica√ß√£o na regi√£o da face e
    retorna apenas resultados que excedem os thresholds de confian√ßa configurados.
    
    Args:
        model_gender (YOLO): Modelo treinado para classifica√ß√£o de g√™nero
        model_emotion (YOLO): Modelo treinado para classifica√ß√£o de emo√ß√£o
        face_region (numpy.ndarray): Imagem da regi√£o facial extra√≠da
        confidence_gender (float): Threshold m√≠nimo de confian√ßa para g√™nero
        confidence_emotion (float): Threshold m√≠nimo de confian√ßa para emo√ß√£o
        
    Returns:
        tuple: (gender, emotion)
            - gender (str or None): G√™nero classificado ou None se baixa confian√ßa
            - emotion (str or None): Emo√ß√£o classificada ou None se baixa confian√ßa
    """
    # Validar entrada - regi√£o facial deve ser v√°lida
    if face_region is None or face_region.size == 0:
        return None, None
    
    gender = None
    emotion = None
    
    # === CLASSIFICA√á√ÉO DE G√äNERO ===
    try:
        # Executar infer√™ncia do modelo de g√™nero
        gender_results = model_gender(face_region, verbose=False)
        
        if len(gender_results) > 0 and hasattr(gender_results[0], 'probs'):
            # Obter confian√ßa da predi√ß√£o principal
            gender_confidence = gender_results[0].probs.top1conf.item()
            
            # Aceitar apenas predi√ß√µes com confian√ßa suficiente
            if gender_confidence >= confidence_gender:
                gender_class_idx = gender_results[0].probs.top1
                gender = CLASSES_GENDER[gender_class_idx]
                
    except Exception as e:
        # Log silencioso - falhas na classifica√ß√£o n√£o devem interromper o sistema
        pass
    
    # === CLASSIFICA√á√ÉO DE EMO√á√ÉO ===
    try:
        # Executar infer√™ncia do modelo de emo√ß√£o
        emotion_results = model_emotion(face_region, verbose=False)
        
        if len(emotion_results) > 0 and hasattr(emotion_results[0], 'probs'):
            # Obter confian√ßa da predi√ß√£o principal
            emotion_confidence = emotion_results[0].probs.top1conf.item()
            
            # Aceitar apenas predi√ß√µes com confian√ßa suficiente
            if emotion_confidence >= confidence_emotion:
                emotion_class_idx = emotion_results[0].probs.top1
                emotion = CLASSES_EMOTION[emotion_class_idx]
                
    except Exception as e:
        # Log silencioso - falhas na classifica√ß√£o n√£o devem interromper o sistema
        pass
    
    return gender, emotion

def detect_faces(model_face, model_gender, model_emotion, frame, confidence_face, confidence_gender, confidence_emotion):
    """
    Pipeline principal de detec√ß√£o e classifica√ß√£o facial.
    
    Esta fun√ß√£o coordena todo o processo:
    1. Detecta faces no frame usando YOLO
    2. Aplica blur para conformidade com LGPD
    3. Classifica g√™nero e emo√ß√£o de cada face
    4. Desenha visualiza√ß√µes (bounding boxes e labels)
    
    Args:
        model_face (YOLO): Modelo para detec√ß√£o de faces
        model_gender (YOLO): Modelo para classifica√ß√£o de g√™nero
        model_emotion (YOLO): Modelo para classifica√ß√£o de emo√ß√£o
        frame (numpy.ndarray): Frame original da c√¢mera/v√≠deo
        confidence_face (float): Threshold para detec√ß√£o de faces
        confidence_gender (float): Threshold para classifica√ß√£o de g√™nero
        confidence_emotion (float): Threshold para classifica√ß√£o de emo√ß√£o
        
    Returns:
        numpy.ndarray: Frame processado com detec√ß√µes e anota√ß√µes visuais
    """
    
    # === ETAPA 1: DETEC√á√ÉO DE FACES ===
    # Executar detec√ß√£o usando o modelo YOLO de faces
    face_results = model_face(frame, verbose=False, conf=confidence_face)
    face_detections = sv.Detections.from_ultralytics(face_results[0])
    
    # Criar c√≥pia do frame para preservar o original
    frame_with_detections = frame.copy()
    
    # === ETAPA 2: APLICA√á√ÉO DE BLUR (CONFORMIDADE LGPD) ===
    # Aplicar blur autom√°tico em todas as faces detectadas para proteger privacidade
    if len(face_detections) > 0:
        blur_annotator = sv.BlurAnnotator(kernel_size=30)  # Blur forte para anonimiza√ß√£o
        frame_with_detections = blur_annotator.annotate(
            scene=frame_with_detections, 
            detections=face_detections
        )
    
    # === ETAPA 3: CLASSIFICA√á√ÉO E ANOTA√á√ÉO ===
    # Processar cada face detectada individualmente
    for i, face_bbox in enumerate(face_detections.xyxy):
        
        # Extrair regi√£o da face do frame original (sem blur) para classifica√ß√£o
        face_region = extract_face_region(frame, face_bbox)
        
        # Classificar g√™nero e emo√ß√£o da face extra√≠da
        gender, emotion = classify_face(
            model_gender, model_emotion, face_region, 
            confidence_gender, confidence_emotion
        )
        
        # === CRIA√á√ÉO DO LABEL DIN√ÇMICO ===
        # Construir label baseado nos resultados da classifica√ß√£o
        face_label = "Face"  # Label padr√£o
        
        if gender and emotion:
            # Ambas classifica√ß√µes dispon√≠veis
            face_label = f"Face: {gender.title()} - {emotion.title()}"
        elif gender:
            # Apenas g√™nero dispon√≠vel
            face_label = f"Face: {gender.title()}"
        elif emotion:
            # Apenas emo√ß√£o dispon√≠vel
            face_label = f"Face: {emotion.title()}"
        
        # === DESENHO DAS ANOTA√á√ïES VISUAIS ===
        # Converter coordenadas da bounding box
        x1, y1, x2, y2 = map(int, face_bbox)
        w, h = x2 - x1, y2 - y1
        
        # Desenhar bounding box com cantos destacados (estilo futurista)
        corner_rect(
            frame_with_detections, 
            (x1, y1, w, h), 
            l=15,  # Comprimento das linhas dos cantos
            t=4,   # Espessura das linhas
            color_rect=(255, 0, 255),    # Magenta para o ret√¢ngulo
            color_corners=(0, 255, 0)    # Verde para os cantos
        )
        
        # Desenhar label com fundo colorido
        put_text_rect(
            frame_with_detections,
            face_label,
            (max(0, x1), max(35, y1)),  # Posi√ß√£o ajustada para n√£o sair da tela
            scale=0.8,
            thickness=2,
            color_rect=(224, 182, 90),   # Fundo azul acinzentado
            color_text=(40, 40, 40),     # Texto preto
            font=cv2.FONT_HERSHEY_DUPLEX,
            offset=5,
        )
    
    return frame_with_detections

def main():
    """
    Fun√ß√£o principal do sistema de detec√ß√£o de g√™nero e emo√ß√£o.
    
    Coordena a inicializa√ß√£o dos modelos, captura de v√≠deo e loop principal
    de processamento em tempo real.
    """
    print("üöÄ Iniciando Sistema de Detec√ß√£o de G√™nero e Emo√ß√£o")
    print("="*50)
    
    try:
        # === INICIALIZA√á√ÉO ===
        # Carregar configura√ß√µes do sistema
        config = load_config()
        
        # Configurar fonte de v√≠deo (webcam ou arquivo)
        source_video = config.get("source_video", 0)
        print(f"üìπ Fonte de v√≠deo: {source_video}")
        
        # Inicializar captura de v√≠deo
        cap = cv2.VideoCapture(source_video)
        if not cap.isOpened():
            raise Exception(f"Erro ao abrir fonte de v√≠deo: {source_video}")
            
        # Configurar resolu√ß√£o da c√¢mera para melhor qualidade
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)
        
        # Carregar todos os modelos YOLO
        model_face, model_gender, model_emotion = load_models()
        
        print("\n‚úÖ Sistema inicializado com sucesso!")
        print("üí° Pressione 'q' para sair")
        print("="*50)
        
        # === LOOP PRINCIPAL DE PROCESSAMENTO ===
        frame_count = 0
        
        while True:
            # Capturar frame da c√¢mera/v√≠deo
            success, frame = cap.read()
            if not success:
                print("‚ö†Ô∏è  Falha na captura do frame ou fim do v√≠deo")
                break
            
            frame_count += 1
            
            # Processar frame: detectar faces e classificar
            frame_with_detections = detect_faces(
                model_face,
                model_gender,
                model_emotion,
                frame,
                config["confidence_face"],
                config["confidence_gender"],
                config["confidence_emotion"]
            )
            
            # === REDIMENSIONAMENTO PARA EXIBI√á√ÉO ===
            resize_resolution = config["resize_resolution"]
            
            # Suportar m√∫ltiplos formatos de resolu√ß√£o
            if isinstance(resize_resolution, str) and 'x' in resize_resolution:
                # Formato "1280x720" - resolu√ß√£o fixa
                try:
                    width_str, height_str = resize_resolution.split('x')
                    resize_width = int(width_str)
                    resize_height = int(height_str)
                    resized = cv2.resize(frame_with_detections, (resize_width, resize_height))
                except ValueError:
                    # Fallback se formato for inv√°lido
                    print(f"‚ö†Ô∏è  Formato de resolu√ß√£o inv√°lido: {resize_resolution}. Usando padr√£o.")
                    resized = cv2.resize(frame_with_detections, (1280, 720))
                    
            elif isinstance(resize_resolution, (int, float)):
                # Formato num√©rico - manter propor√ß√£o baseada na largura
                resize_width = int(resize_resolution)
                original_height, original_width = frame_with_detections.shape[:2]
                resize_height = int(original_height * resize_width / original_width)
                resized = cv2.resize(frame_with_detections, (resize_width, resize_height))
                
            else:
                # Fallback para casos n√£o previstos
                print(f"‚ö†Ô∏è  Tipo de resolu√ß√£o n√£o suportado: {type(resize_resolution)}. Usando padr√£o.")
                resized = cv2.resize(frame_with_detections, (1280, 720))
            
            # Exibir frame processado
            cv2.imshow("Gender & Emotion Detection System", resized)
            
            # === CONTROLE DE SA√çDA ===
            # Verificar se usu√°rio pressionou 'q' para sair
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                print(f"\nüëã Encerrando sistema... (Processados {frame_count} frames)")
                break
                
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Sistema interrompido pelo usu√°rio")
    except Exception as e:
        print(f"\n‚ùå Erro no sistema: {e}")
    finally:
        # === LIMPEZA E FINALIZA√á√ÉO ===
        # Liberar recursos da c√¢mera
        if 'cap' in locals():
            cap.release()
            
        # Fechar todas as janelas OpenCV
        cv2.destroyAllWindows()
        
        print("üîÑ Recursos liberados. Sistema encerrado.")

if __name__ == "__main__":
    # Ponto de entrada do programa
    # Executa apenas se o arquivo for executado diretamente (n√£o importado)
    main()
